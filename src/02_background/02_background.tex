\section{Background}%
\label{sec:background}

\subsection{Allen's Interval Algebra}%
\label{sub:allen_interval_algebras}

The concept of Allen's interval algebras was first introduced in \cite{allen83}. The main
idea was to introduce a new system for arguing about time intervals in a qualitative way, similar
to how humans think about time. For example, when retelling a story
people will convey some ordering of events' start and end times, while skipping over the actual
figures. Now, this might happen because the specific time frames aren't relevant to the story
or even because they are not known. Regardless of the reason, it is often helpful to talk 
about time without being overly explicit. Computers however, tend to argue about time in a
quantitative manner: saving timestamps inside of logs, such that if something goes wrong, the
order of events can be gotten; checking the system clock to tell if access tokens have expired;
so on. The difficulties of expressing time qualitatively on a computer become even more obvious
when working towards artificial intelligence, for instance in natural language processing,
where the computer must be able to infer the timing of events from people's informal speech or
writing.

Interval algebras were Allen's approach to have computers reasoning about time as humans did and
still do. In an interval algebra, time intervals are treated as primitives, with binary relations
recording their ordering and level of overlap. There are 13 basic
relations which allow one to describe fully how any two intervals $I$ and $J$ might relate. The
relations and their meaning can be found in \cref{tab:basic_relations}. Apart from $=$ which is
its own dual relation, relations come in pairs of dual relations: as an example, if it is known
that $I < J$, then it can be immediately infered that $J > I$. Due to this
duality we have omitted the meaning of the dual relations from \cref{tab:basic_relations} to save
space. We have also used $\istart{I}$ and $\iend{I}$ as shorthands for the
start and end points of the interval $I$.

\begin{table}[htpb]
  \centering
  \begin{tabular}{|l|c|c|c|}
    \hline
    Relation & Symbol & Dual Symbol & Meaning \\
    \hline
    \(I\) starts \(J\)
             & \(I < J\)
             & \(I > J\)
             & \(\istart{I} < \iend{I} < \istart{J} < \iend{J}\)\\
    \(I\) meets \(J\)
             & \(I\text{ m } J\)
             & \(I\text{ M } J\)
             & \(\istart{I} < \iend{I} = \istart{J} < \iend{J}\)\\
    \(I\) overlaps \(J\)
             & \(I\text{ o } J\)
             & \(I\text{ O } J\)
             & \(\istart{I} < \istart{J} < \iend{I} < \iend{J}\)\\
    \(I\) starts \(J\)
             & \(I\text{ s } J\)
             & \(I\text{ S } J\)
             & \(\istart{J} < \istart{I} < \iend{I} < \iend{J}\)\\
    \(I\) finishes \(J\)
             & \(I\text{ f } J\)
             & \(I\text{ F } J\)
             & \(\istart{I} = \istart{J} < \iend{I} < \iend{J}\)\\
    \(I\) during \(J\)
             & \(I\text{ d } J\)
             & \(I\text{ D } J\)
             & \(\istart{J} < \istart{I} < \iend{I} = \iend{J}\)\\
    \(I\) equals \(J\)
             & \(I = J\)
             & \(I = J\)
             & \(\istart{I} = \istart{J} < \iend{I} = \iend{J}\)\\
    \hline
  \end{tabular}
  \caption{13 Basic Relations of Allen's Interval Algebra}
  \label{tab:basic_relations}
\end{table}

The choice of these 13 relations has some advantages which simplify reasoning, namely the
relations are both exhaustive and mutually-exclusive.  By exhaustive we mean that for any two
intervals $I$ and $J$, there exists at least one relation R  such that $I\text{ R }J$. By
mutually exclusive we mean there exists at most one such relation R for any two intervals $I$ and 
$J$. 

Looking at the intended meaning of each relation in \cref{tab:basic_relations} it seems that time
points are not considered as valid intervals since $\istart{I}$ is always strictly less than
$\iend{I}$. This is no accident, as time points can cause ambiguities and thus complicate the
semantics of interval algebras. This deserves some attention, since in natural language we often
refer to actual points in time instead of intervals.  For example the phrase ``I caught the ball''
suggests that the act of catching the ball was instantaneous, even though it definitely
happened over an interval of time, simply a very short one. In reality, whether we consider
certain intervals as time points or as actual intervals depends a lot on context. Furthermore,
if one really wishes to deal with time points, then they can be modelled by suitably small
intervals.

\subsubsection{Basic Algorithm}%
\label{ssub:basic_algorithm}

With some of the main ideas explained, now is a good time to see the basic algorithm described in
\cite{allen83}. It takes as input a collection of intervals and their known relations, and
attempts to infer as many of the missing relations as possible. As mentioned
before, time intervals are primitives in Allen's interval algebra and since every two intervals
must be related by one of the symbols in \cref{tab:basic_relations}, directed graphs with
labeled edges are a good representation of this information. In such a graph, there will be a node
for each time interval we are interested in, and each edge will be labelled with the sets of
symbols that describe the possible relations between the source and target intervals. Considering
an illustrative example, suppose there are two nodes $I$ and $J$ with an edge from $I$ to $J$
labelled by ``<mo'': this should be read as saying that one of $I < J$, $I\text{ m }J$ or
$I\text{ o }J$ is expected to hold, although we do not know which one.
This labelling takes advantage of the fact that all the basic relations
are mutually exclusive, so the implied disjunctions in the above notation cause no ambiguities.
Similarly, the fact that the relation symbols are exhaustive also has a consequence for this
representation: naÃ¯vely, we would expect the interval graph to be a complete directed graph,
but as the 13 basic relations all have a dual, it is always possible to tell what the label from
$I$ to $J$ should be by reading the label from $J$ to $I$. As a result, with some care, one can use
an undirected graph, allowing for some saved space.

Before seeing the main algorithm
it will be helpful to define a helper function: when given two relation
symbols $r1$ and $r2$ linking three intervals by $I\ r1\ J$ and $J\ r2\ K$, it is important to tell
what constraints there are on the possible relations between $I$ and $K$. 
This is done by looking up the relevant entry $T(r1,r2)$ of
\cref{tab:transitivity}. Next, given a pair of edges from $I$ to $J$ and $J$ to $K$, each
labelled by the strings $R1$ and $R2$, we let $\textit{Constraints}(R1,R2)$ be the minimum set of
relation symbols which could relate $I$ and $K$. The pseudocode for computing this can be found in
\cref{alg:constraints}.

\begin{table}[htpb]
  \centering
  \input{02_background/transitivity_table.tex}
  \caption{Transitivity table for basic relations -- adapted from \cite{thomaswebsite}.\\
  Given relations $I\ r1\ J$ and $J\ r2\ K$,
  the possible relations between $I$ and $K$ will be under row $r1$ and column $r2$. Entries
  with the word "full" should contain all relations and entries labelled "con" should contain
  "o s f d = O S F D".}
  \label{tab:transitivity}
\end{table}

\begin{algorithm}
  \caption{Computing constraints given two edge labels. \cite{allen83}}\label{alg:constraints}
  \begin{algorithmic}
    \Function{Constraints}{R1, R2}
    \State $C \gets \emptyset$
    \ForAll{$r1 \in R1$}
      \ForAll{$r2 \in R2$}
        \State $C \gets C \cup T(r1,r2)$
      \EndFor
    \EndFor
    \State \Return C
    \EndFunction
  \end{algorithmic}
\end{algorithm}

As for the actual algorithm in question,
the pseudocode to update a temporal network with a new label for a specific edge can be
seen in \cref{alg:main_algo}. It is assumed there exists a \textit{ToDo} queue
where where we place edges whose constraints need to be updated. Furthermore, for
intervals $i$ and $j$, we let $N(i,j)$ denote the label on the edge from $i$ to $j$ in the interval
graph and use $R(i,j)$ to denote the new label for the edge from $i$ to $j$.
Lastly, in simpler cases the \textit{Comparable} function can be taken to always return true, but
as the number of intervals grows, it is helpful to reduce the amount of updates needed through some
optimisations explained in chapter 5 of \cite{allen83}.

\begin{algorithm}
  \caption{Updating temporal network. \cite{allen83}}\label{alg:main_algo}
  \begin{algorithmic}
    \Procedure{To Add}{R(i, j)}
      \State Add $(i,j)$ to queue \textit{ToDo}
      \While{\textit{ToDo} is not empty}
        \State Get next $(i,j)$ from queue \textit{ToDo}
        \State $N(i,j) \gets R(i,j)$
        \ForAll{nodes $k$ such that \Call{Comparable}{k, j}}
          \State $R(k, j) \gets N(k,j) \cap \Call{Constraints}{N(k,j), R(i,j)}$
          \If{$R(k,j) \subset N(k,j)$} %TODO check if this should be j or i
            \State Add $(k,j)$ to \textit{ToDo}
          \EndIf
        \EndFor
        \ForAll{nodes $k$ such that \Call{Comparable}{i, k}}
          \State $R(i, k) \gets N(i,k) \cap \Call{Constraints}{R(i,j), N(j,k)}$
          \If{$R(i,k) \subset N(i,k)$}
            \State Add $(i,k)$ to \textit{ToDo}
          \EndIf
        \EndFor
      \EndWhile
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

With an inference algorithm such as this, the question of correctness is quite important.
Thankfully, if given a satisfiable network of intervals, this algorithm will never infer any
erroneous constraints. However, if given an unsatisfiable network, then it is not guaranteed that
the algorithm will detect the unsatisfiability. Since we only consider paths of length 2, any 
inconsistencies detected must occur within a 3 node subgraph and inconsistencies from larger
subgraphs will be missed. If one is unsure about the satisfiability of the input graph though,
then a search for valid assignments can be made after running the inference algorithm. It will
still be worth it to run the inference algorithm in this case since the extra added constraints
might make it a lot faster to check for valid assignments.

\newpage

\subsection{Model Theory}%
\label{sub:model_theory}

We assume some familiarity with the basics of model theory, for which we recommend \cite{marker02}.
In this section we will focus mainly on the definitions which might not feature in an introductory
course in model theory, but are needed to understand the work done in \cref{sec:model_theory}.

First we introduce the idea of homogeneous structures, and how to construct examples of these
using FraÃ¯ssÃ© limits. These structures will be characterised by their ability to extend isomorphisms
between finite substructures to automorphisms of the whole structure. As a result, homogeneous
structures can have very interesting
automorphism groups and their study helps link model theory, group theory and combinatorics. For a
thorough survey of the area see \cite{macpherson11}.

Then we will cover some important classes of theories arising from classification theory, namely we
will introduce stable theories and one of their possible generalisations, NIP theories. 
The concept of stable theories arose to study the spectrum of complete theories, offering a
definitive answer through the use of tools like forking and dividing. NIP theories generalise the
class of stable theories to include important examples like linear orders and geometric examples
like algebraically closed valued fields \cite{acvf-NIP} or the real exponential field
\cite{steinhorn1999}.

\subsubsection{Homogenous Structures and FraÃ¯ssÃ© Classes}%
\label{ssub:homogeneous_structures_and_fraisse_classes}

We work with the definition of a homogenous structure found in \cite{macpherson11} as we are
mainly interested in the study of FraÃ¯ssÃ© limits.

\begin{defn}
  A homogenous structure is a countable, possibly finite, relational structure (with finite
  language $\lang$) such that, for every isomorphism $f : U \to V$ between finite substructures
  $U, V \subseteq M$, there is an authomorphism $f' : M \to M$ on $M$ extending $f$.
\end{defn}

The simplest example of a homogeneous structure comes from the theory of strict linear orders, which
we now define.

\begin{defn}
  We define the language of strict linear orders $\lslo$ as the single binary relation $\{ < \}$.
\end{defn}

\begin{defn}
  We define the theory of strict linear orders as
  \begin{align*}
    \tslo = \big\{\; & \forall a, \lnot\ (a < a), \\
              & \forall a, \forall b, \forall c, (a < b) \land (b < c) \rightarrow (a < c) \\
              & \forall a, \forall b, (a < b) \lor (a = b) \lor (b < a) \;\big\}
  \end{align*}
\end{defn}

So a strict linear order consists of a binary relation on a set, which is irreflexive, transitive
satisfies the trichotomy condition. Notice that from these we can infer that the binary relation
must be antisymmetric, since if $a < b$ and $b < a$ then by transitivity $a < a$, which 
contradicts the irreflexivity of $<$.

\begin{prop}
  The strict linear order $(\Q,<)$ is homogeneous.
\end{prop}
\begin{proof}
  First we see how to expand the
  domain of an order isomorphism $f : L \to P$ with $L, P \subseteq \Q$ both finite. Suppose that we
  wish to extend the domain of $f$ to include some $a \in \Q \setminus L$. There are three cases to
  worry about here:
  \begin{itemize}
    \item If $a$ is an upper bound for $L$, we fix some upper bound $b$ of $P$ which is
      not in $P$. Such a $b$ must exist as $\Q$ is unbounded and $P$ is finite. Then, we extend
      $f : L \to P$ to $g : L \cup \{a\} \to P \cup \{b\}$ by sending $g(a) = b$. This remains an
      order isomorphism since for any $x \in P$, we must have $x < a$ and $g(x) < b = g(a)$ since
      $a$ and $b$ are upper bounds of $L$ and $P$ respectively.
    \item If $a$ is a lower bound for $L$, then we fix a lower bound $b$ of $P$ not already in
      $P$ and extend $f$ to $g : L \cup \{a\} \to P \cup \{b\}$ by sending $a$ to $b$ again.
      Dually to the upper bound case, $a$ and $b$ are lower bounds of the domain and
      codomain, hence $g$ remains an order isomorphism.
    \item If $a$ is neither an upper nor lower bound, then we notice that $L,P$ are finite linear
      orders and hence discrete.
      This means that we can find $a_1,a_2 \in L$ such that $a_1 < a < a_2$ and for no $x \in L$ do
      we have $a_1 < x < a_2$. Now we can fix some $b \in \Q \setminus P$ such that
      $g(a_1) < b < g(a_2)$ since $\Q \setminus P$ is still dense. We extend $f$ to
      $g : L \cup \{a\} \to P \cup \{b\}$ by sending g(a) = b. This remains an order isomorphism
      since for any $x \in L$ we have either $x \leq a_1 < a$, so
      \begin{equation*}
        g(x) = f(x) \leq f(a_1) = g(a_1) < b = g(a)
      \end{equation*}
      or we have $a < a_2 \leq x$, in which case
      \begin{equation*}
        g(a) = b < g(a_2) = f(a_2) \leq f(x) = g(x)
      \end{equation*}
  \end{itemize}
  If we wanted to expand the codomain of $f$ then it suffices to extend the domain of 
  $f^{-1}$ to include whichever
  element we needed, giving us a function $g$. The inverse $g^{-1}$ is then the required extension
  of $f$.

  Now, fix two finite suborders $L,P \subseteq \Q$
  and suppose we have some order isomorphism $f : L \to P$. To extend $f$ to an automorphism of $\Q$
  we start by fixing an enumeration $(a_1, a_2, \dots)$ of the elements of $\Q$ and we define
  three sequences: two sequences
  $(L_1, L_2, \dots)$ and $(P_1, P_2, \dots)$ of increasing subsets of $\Q$; and a sequence 
  $(g_1, g_2, \dots)$ of bijections $g_i : L_i \to P_i$ where each $g_i$ extends its predecessors.
  These sequences are defined inductively by:
  \begin{itemize}
    \item $k = 1$: let $L_1 = L,\ P_1 = P$ and $g_1 = f$.
    \item $k = 2l$ for $l \in \{1, 2, \dots\}$: at even indices we focus on increasing the domain of
      $g_i$ to all of $\Q$. If $a_l \in L_{k-1}$ already then we let
      \begin{align*}
        L_k & = L_{k-1} \\
        P_k & = P_{k-1} \\
        g_k & = g_{k-1}
      \end{align*}
      Otherwise we extend $g_{k-1}$ to an order isomorphism
      $h : L_{k-1} \cup \{a_l\} \to P_{k-1} \cup \{b\}$ for some appropriate $b$ and let
      \begin{align*}
        L_k & = L_{k-1} \cup \{a_l\} \\
        P_k & = P_{k-1} \cup \{b\} \\
        g_k & = h
      \end{align*}
    \item $k = 2l + 1$ for $l \in \{1, 2, \dots\}$: at odd indices we focus on increasing the
      codomain of $g_i$ to all of $\Q$. If $a_l \in P_{k-1}$ already then we let
      \begin{align*}
        L_k & = L_{k-1} \\
        P_k & = P_{k-1} \\
        g_k & = g_{k-1}
      \end{align*}
      Otherwise we extend $g_{k-1}$ to an order isomorphism
      $h : L_{k-1} \cup \{b\} \to P_{k-1} \cup \{a_l\}$ for some appropriate $b$ and let
      \begin{align*}
        L_k & = L_{k-1} \cup \{b\} \\
        P_k & = P_{k-1} \cup \{a_l\} \\
        g_k & = h
      \end{align*}
  \end{itemize}
  We must have $\bigcup L_k = \bigcup P_k = \Q$ since each $x \in \Q$ must appear as $a_l$
  in our enumeration for some $l \in \{1, 2, \dots\}$. So $x \in L_{2l}$ and $x \in P_{2l+1}$
  implying that $x \in \bigcup L_k$ and $x \in \bigcup P_k$. Since each $g_k$ is an isomorphism,
  their union $g = \bigcup g_k$ must also be and by construction $g$ will extend $f$.
\end{proof}

The main bulk of the work above comes from the density of $\Q$, in fact, non dense linear orders are
only homogeneous in the trivial case.

\begin{prop}
  If $I$ is a non-dense strict linear order with more than one element, then $L$ is not homogeneous.
\end{prop}
\begin{proof}
  If the linear order is $\{x < y\}$ then the map partial isomorphism
  $x \mapsto y$ cannot be extended further.

  If the linear order has more than 3 elements, we can either find $x < y < z$ or
  $z < x < y$ where $x$ and $y$ have no elements inbetween (as the order is not dense). In the first
  case, the partial order isomorphism
  \begin{equation*}
    f : \{x,z\} \to \{x,y\} \text{ sending } x \mapsto x \text{ and } z \mapsto y.
  \end{equation*}
  cannot be extended to add $y$ since there is no $w$ such that $f(x) = x < w < y = f(z)$.
  By a similar argument, the following map handles the second case
  \begin{equation*}
    f : \{y,z\} \to \{x,y\} \text{ sending } y \mapsto y \text{ and } z \mapsto x.
  \end{equation*}
\end{proof}

\begin{cor}
  $\N$ and $\Z$ are not homogeneous.
\end{cor}

Homogeneous structures can seem quite mysterious at first, so
it would be interesting to see how to build homogeneous models of a theory. Towards this goal,
fix some relational language $\lang$ and structure $M$ and consider the class of finite
$\lang$-structures embeddable intro $M$.
This will be called the age of the structure $M$, denoted $\age(M)$. In general, the age of any
countable relational structure will satisfy the following three properties.

\begin{defn}
  A class $\C$ has the hereditary property (HP) if it is closed under substructures, so if
  $A, B$ are $\lang$-structures, $A \in \C$ and $B \subseteq A$ then $B \in \C$ too.
\end{defn}

\begin{defn}
  A class $\C$ has the joint embedding property (JEP) if for any $A,B \in \C$, we can find a
  third $\lang$-structure $C \in \C$ such that $A$ and $B$ both embed into it.
  language $\lang$).
\end{defn}

\begin{defn}
  A class $\C$ is essentially countable (EC) if, up to isomorphism, there are only countably many
  $\lang$-structures.
\end{defn}

Given a class $\C$ satisfying the above three properties, one may construct a countable model
$M$ such that $\age(M) = \C$ by enumerating all the isomorphism classes in $\C$ and gluing all of
these in order using the JEP. Suppose that $M$ is homogeneous though, then we can say something
further about its age, namely it will satisfy the following.

\begin{defn}
  A class $\C$ has the amalgamation property (AP) if for any span
  \begin{equation*}
    A \longleftarrow C \longrightarrow B
  \end{equation*}
  with $A,B,C \in \C$, there exists some $\lang$-structure
  $\Omega \in C$ with embeddings
  \begin{equation*}
    A \longrightarrow \Omega \longleftarrow B
  \end{equation*}
  making the following diagram commute:
  \[\begin{tikzpicture}
    \node[minimum size=7mm] (C)              {$C$};
    \node[minimum size=7mm] (B) [right=of C] {$B$};
    \node[minimum size=7mm] (A) [below=of C] {$A$};
    \node[minimum size=7mm] (O) [right=of A] {$\Omega$};
    \draw[->] (C.east) -- (B.west);
    \draw[->] (C.south) -- (A.north);
    \draw[->] (B.south) -- (O.north);
    \draw[->] (A.east) -- (O.west);
  \end{tikzpicture}\]
\end{defn}

Considering again a class of models $\C$ satisfying the HP, EC, and JEP, suppose further that that
$\C$ also has the AP. Then a theorem of FraÃ¯ssÃ© tells us that gluing all of the models in $\C$
yields a homogeneous model $M$ with $\age(M) = \C$.

\begin{thm}[FraÃ¯ssÃ©'s theorem]
  Given a class $\C$ of finite $\lang$-structures satisfying the properties HP, JEP, AP and EC, then
  there exists some homogeneous $\lang$-structure $M$ such that $\age(M) = \C$. Furthermore, if we
  have two such homogeneous $\lang$-structures $M$ and $N$, then necessarily $M \cong N$.
\end{thm}

A proof of this, along with proofs that $\age(M)$ satisfies the relevant properties can be found in
\cite{hodges93}. When the above structure $M$ exists for a class $\C$, then we call $M$ the FraÃ¯ssÃ©
limit of $\C$.

Note that the uniqueness condition of the FraÃ¯ssÃ© limit only applies to homogeneous
structures. In fact, if we consider the class of finite strict linear orders, we can see it
coincides with $\age(\Q)$, $\age(\Z)$ and also $\age(\N)$ despite neither of these linear orders
being isomorphic. We saw earlier that neither $\Z$ nor $\N$ were homogeneous though, hence why this
happens.

As we have decided not to show a lot of proofs for this section, we will instead consider in
detail how to compute the FraÃ¯ssÃ© limit of the class $\finslo$ of finite strict linear orders,
to hopefully ground all these definitions a bit better. First we need to see this limit exists
by checking that $\finslo$ satisfies the 4 properties needed to apply FraÃ¯ssÃ©'s theorem.

First we prove some general results about the HP and EC:

\begin{prop}
  Given a relational, universal theory $\theory$, the class of finite models of $\theory$ satisfies
  the hereditary property.
\end{prop}
\begin{proof}
  Fix some finite model $M \models \theory$ and a substructure
  $N \subseteq M$. Since $M$ was finite, $N$ must also be finite. Now, suppose we have a universal
  sentence
  \begin{equation*}
    \phi = \forall x_1, \dots, \forall x_n, \psi(x_1,\dots,x_n)
  \end{equation*}
  where $\psi$ is a quantifier free formula such that $M \models \phi$. Fixing some tuple $(a_1,\dots,a_n) \in N^n$ we
  know that $M \models \psi(a_1,\dots,a_n)$, but $N$ is a substructure of $M$ and the truth value
  of quantifier free formulas is preserved by embeddings, so $N \models \psi(a_1,\dots,a_n)$ too.
  As we fixed an arbitrary tuple of elements in $N$, this means that $N \models \phi$.
  The theory $\theory$ is taken to be universal, hence all sentences $\phi \in \theory$ are
  equivalent to some universal sentence modulo $\theory$. As we just saw, universal sentences
  are preserved under taking substructures, so $\phi$ must be preserved too.
  Hence if $M \models \theory$, then any substructure
  $N \subseteq M$ will also be a model of $\theory$.
\end{proof}

\begin{cor}
  The class $\finslo$ has the HP.
\end{cor}

\begin{prop}
  For any finite language $\lang$, the class of all finite $\lang$-structures is essentially
  countable.
\end{prop}
\begin{proof}
  Fix some finite language
  \begin{equation*}
    \lang = \{c_1, \dots, c_m, f_1, \dots, f_n, R_1, \dots, R_l \}
  \end{equation*}
  where the $c_i$ are constant symbols, the $f_i$ are function symbols and the $R_i$ are relation
  symbols. We will show that the class of finite $\lang$-structures must be essentially countable.
  First, consider the initial segment $M = \{1,\dots,r\} \subset \N$. If we wish to turn $M$ into a
  $\lang$-structure then we must pick interpretations for all the symbols in $\lang$. For a constant
  symbol this consists of picking a single element, so there are $r$ possible options. For a
  function symbol with $a$-arity, we need to pick an element of $M$ for every input tuple of $a$
  elements of $M$, so there are $r^{r^a}$ possibilities. Similarly, for a relation symbol $R$ with
  $b$-arity, we need to pick a truth value for all tuples of size $b$ in $M$, so there are $2^{r^b}$
  options. Hence, if we denote by $a_i$ the arity of the symbol $f_i$ and $b_i$ the arity of the
  symbol $R_i$, then the total number of distinct $\lang$-structures on $M$ is finite, more
  specifically it is
  $r^{\left(m + \sum_{i=1}^{n} r^{a_i}\right)}2^{\left(\sum_{i=1}^{l} r^{b_i}\right)}$.
  As such, the set of
  possible $\lang$-structures on all initial segments $\{1,\dots,r\} \subseteq \N$ is countable.
  Every finite $\lang$-structure must be isomorphic to at least one of these, so the class of finite
  $\lang$-structures is essentially countable too.
\end{proof}

\begin{cor}
  The class $\finslo$ is EC.
\end{cor}
\begin{proof}
  The class $\finslo$ is a subclass of all finite strict linear orders, hence if the latter is EC,
  then the former must also be EC.
\end{proof}

Now we focus solely on strict linear orders.

\begin{prop}
  The class $\finslo$ has the joint embedding property.
\end{prop}
\begin{proof}
  Given two strict linear orders $L,P$, we can turn their disjoint union $L \sqcup P$ into a
  strict linear order by carrying over the orderings from $L$ and $P$ and setting
  $x < y$ for all $x \in L$ and $y \in P$. Then, the usual injections into the disjoint union
  $f : L \to L \sqcup P$ and $g : P \to L \sqcup P$ are order preserving maps. If
  $L$ and $P$ are both finite, so will $L \sqcup P$ be, hence the class of finite strict
  linear orders has the joint embedding property.
\end{proof}

\begin{prop}
  The class $\finslo$ has the amalgamation property.
\end{prop}
\begin{proof}
  Given linear orders $A,L,P$ and order embeddings $f : A \to L$, $g : A \to P$, we wish to
  find a linear order $\Omega$ and order embeddings $f' : L \to \Omega$, $g' : P \to \Omega$ such
  that $f' \circ f = g' \circ g$. If any of $A,L,P$ is empty, we devolve to the previous proof,
  so we may assume that $A,L$ and $P$ are all non-empty. We take the product $L \times P$ and
  order it lexicographically, such that for all $(x,y), (x',y') \in L \times P$
  \begin{equation*}
    (x,y) < (x',y') \iff (x < y') \lor ((x = x') \land (y < y'))
  \end{equation*}
  Fixing some $y \in P$ we define $f' : L \to L \times P$ by
  \begin{equation*}
    f'(x) = \begin{cases}
      (x, g(a)) & \text{ if} f^{-1}(x) = \{a\} \\
      (x, y)    & \text{ if } f^{-1}(x) = \emptyset
    \end{cases}
  \end{equation*}
  This preserves the ordering of $L$ as it coincides with the identity map on the first coordinate
  and we are ordering $L \times P$ lexicographically.

  Defining $g' : P \to L \times P$ is slightly trickier. First, we must pick some $y_{L,U} \in L$
  such that $L \leq y_{L,U} \leq U$ for all subsets $L,U \subseteq L$ with $L < U$. Once
  we have fixed our choices, for any $x \in P$ we denote
  $y_x =  y_{\left\{f(a)\ |\ g(a) < x\right\}, \left\{f(a)\ |\ x < g(a)\right\}}$.
  Finally, we define
  \begin{equation*}
    g'(x) = \begin{cases}
      (f(a), x) & \text{ if} g^{-1}(x) = \{a\} \\
      (y_x, x)  & \text{ if } f^{-1}(x) = \emptyset
    \end{cases}
  \end{equation*}
  To check this preserves the ordering of $P$, we fix two elements $x < x'$ of $P$, then:
  \begin{itemize}
    \item If $g^{-1}(x) = \{a\}$ and $g^{-1}(x') = \{b\}$, then $a < b$, which implies that
      $f(a) < f(b)$, so $g'(x) = (f(a),x) < (f(b),x') = g'(x')$.
    \item If $g^{-1}(x) = \{a\}$ and $g^{-1}(x') = \emptyset$, then we have picked $y_x$ such that
      $f(a) < y_x$ so $g'(x) = (f(a),x) < (y_x, x) = g'(x')$.
    \item If $g^{-1}(x) = \emptyset$ and $g^{-1}(x') = \{a\}$, then we have picked $y_x$ such that
      $y_x < f(a)$ so $g'(x) = (y_x, x) < (f(a),x) = g'(x')$.
    \item If $g^{-1}(x) = g^{-1}(x') = \emptyset$ then notice that $y_x \leq y_{x'}$ since
      either there exists some $a \in A$ such that $x < g(a) < x'$, so then
      $y_x \leq g(a) \leq y_{x'}$ or no such $a$ exists and $y_x = y_{x'}$. Thus,
      $g'(x) = (y_x,x) < (y_{x'}, x') = g'(x')$.
  \end{itemize}
  Now that we have the required order embeddings into $\Omega = L \times P$, we just check the
  necessary commutativity condition:
  \begin{equation*}
    f' \circ f(a) = f'(f(a)) = (f(a),g(a)) = g'(g(a)) = g' \circ g(a)
  \end{equation*}
  In the case that $A,L,P$ are finite, then $L \times P$ will also be finite, so this
  shows that the class of finite strict linear orders has the amalgamation property.
\end{proof}

\begin{thm}
  The FraÃ¯ssÃ© limit of $\finslo$ is $\Q$ with its usual order.
\end{thm}
\begin{proof}
  As $\finslo$ satisfies the HP, JEP, AP and is EC, then it must have a FraÃ¯ssÃ© limit $M$. We saw
  before that $\Q$ is homogeneous. By uniqueness of the FraÃ¯ssÃ© limit, it suffices to show that
  $\age(\Q) = \finslo$. Clearly $\age(\Q) \subseteq \finslo$ since
  a suborder of a linear order must still be linear. To see that $\finslo \subseteq \age(\Q)$ we fix
  some finite linear order $L$, then there is an order preserving isomorphism from $L$ to an initial
  segment of $\N$. The inclusion $\N \in \Q$ means this isomorphism realises $L$ as a finite
  suborder of $\Q$.
\end{proof}

\subsubsection{Stable Theories}%
\label{ssub:stable_theories}

\begin{defn}
  Let $\theory$ be a complete theory. For an infinite cardinal $\kappa$, $\theory$ is
  $\kappa$-stable if for every every model $M$ of $\theory$ and subset $A \subseteq M$ with 
  cardinality
  $|A| = \kappa$, then the set of complete $n$-types in $M$ over $A$,  $S_n^M(A)$, has cardinality
  $\kappa$.
\end{defn}

If a theory $\theory$ is $\kappa$-stable for any infinite cardinal $\kappa$, then we call it stable,
otherwise it is called unstable.
Given a model $M$, we say that $M$ is stable (resp. unstable) if the full theory of $M$
is stable (resp. unstable).

The following theorem gives us a characterisation of stability in terms of linear orders, which can
be simpler to reason with, especially when one considers the close relation between linear orders
and interval algebras.

\begin{thm}\label{thm:stable_order_property}
  Let $\theory$ be a complete theory, then $\theory$ is stable if and only
  if there exists a formula $\phi(v_1,\dots,v_n;w_1,\dots,w_n)$ and a model $M \models \theory$ with
  a sequence $x_1,x_2,\dots \in M^n$ such that
  \begin{equation*}
    M \models \phi(x_i;x_j) \iff i < j
  \end{equation*}
  Such a formula is said to have the order property.
\end{thm}

The proof for this is somewhat involved, so we refer the interested reader to \cite{marker02}.

\begin{cor}
  A linear order $L$ is stable if and only if it is finite.
\end{cor}

\begin{exmp}
  A complete theory is strongly minimal if for all models $M$, any definable set (with parameters)
  $D \subseteq M$ is either finite or cofinite. This turns out to be a very strong requirement,
  meaning that strongly minimal theories must also be stable. The proof for this relies on the
  equivalence between stability of a theory and non-existence of a formula with the strict order
  property (which is slightly different from the order property), and so is omitted.

  We know from \cite{marker02} that the following are strongly minimal and hence stable:
  \begin{itemize}
    \item The theory of algebraically closed fields in characteristic $p$ in the language of rings:
      all definable sets of an algebraically closed field $k$ are boolean combinations of zero sets
      of polynomials in $k[x]$. Since these zero sets are either finite or all of $k$, then the
      claim of strong minimality follows.
    \item The theory of $\Q$-vector spaces in the language of modules: for a $\Q$-vector space $V$,
      all definable sets $D \subseteq V$ are given by boolean combinations of formulas of the form
      $nx = a$ where $n \in \N$, and $a \in V$. If $a$ is nonzero, such a formula can have at most
      one solution, hence $D$ will have to be either finite or cofinite.
  \end{itemize}
\end{exmp}

\subsubsection{NIP Theories}
\label{ssub:nip_theories}

\begin{defn}
  For a complete theory $\theory$, we say that a formula $\phi(x;y)$ has the independence property
  if there is a model $M \models \theory$ and sequences $(a_i)_{i<\omega}$,
  $(b_I)_{I \subseteq \omega}$ in $M$ such that
  \begin{equation*}
    M \models \phi(a_i,b_I) \iff i \in I
  \end{equation*}
  If a formula does not have the IP, we say it has the non-independence property (NIP).
\end{defn}

\begin{defn}
  A complete theory has the IP if there exists some formula with the IP. It has the NIP if all
  formulas have the NIP.
\end{defn}

By compactness, to show that a specific formula has the IP, it suffices to consider arbitrarily
large finite sequences.

\begin{prop}
  For a complete theory $\theory$, a formula $\phi(x;y)$ has the IP if and only if the following
  is satisfiable
  \begin{equation*}
    \theory \cup \{\phi(a_i,b_I)\ |\ i \in I \subseteq \{0,1,\dots,n\}\}
  \end{equation*}
  for arbitrarily large $n$ (where the $a_i$ and $b_I$ are new constant symbols).
\end{prop}
\begin{proof}
  The forwards implication follows by taking the model and sequences $(a_i)_{i<\omega}$,
  $(b_I)_{I \subseteq \omega}$ which realise the IP for $\phi(x;y)$ and discarding the
  $a_i$ with $i > n$ and $I \nsubseteq \{0,1,\dots,n\}$.

  For the converse, showing that $\phi(x;y)$ has the IP amounts to showing
  the satisfiability of
  \begin{equation*}
    \theory \cup \{\phi(a_i,b_I)\ |\ i \in I \subseteq \N \}
  \end{equation*}
  But by compactness, it suffices to show satisfiability of
  \begin{equation*}
    \theory \cup \{\phi(a_{i_1},b_{I_1}), \phi(a_{i_2},b_{I_2}) \dots, \phi(a_{i_m},b_{I_m})\}
  \end{equation*}
  where $i_k \in I_k$ for all $k$. Letting $n = \max(i_1,i_2,\dots,i_m)$ and applying our
  hypothesis, we see this is indeed satisfiable.
\end{proof}

This means that if a theory has the NIP, then for all formulas $\phi(x;y)$ and all models $M$,
there exists a maximum $n \in \N$ such that for all $\{a_1, \dots, a_n\} \subseteq M$, there exists
some subset of $\{a_1, \dots, a_n\}$ which we cannot pick out with $\phi(x;y)$ regardless of the
parameter $y$.

Being a generalisation of stable theories, we expect all stable theories to also have the NIP.

\begin{prop}
  All stable theories have the NIP.
\end{prop}
\begin{proof}
  Suppose we have a theory $\theory$ which has the IP, so the IP is realised for some formula
  $\phi(x;y)$ by a model $M \models \theory$ and elements $(a_i)_{i<\omega}$,
  $(b_I)_{I \subseteq \omega}$. Then the formula
  \begin{equation*}
    \psi(x,x';y,y') = \phi(x;y')
  \end{equation*}
  is unstable, since if the sequence $(c_i)_{i<\omega}$ given by $c_i=(a_i,b_{\{n | n < i\}})$ is
  such that
  \begin{equation*}
    M \models \psi(c_i;c_j) \iff
      M \models \phi(a_i; b_{\{n|n < j\}}) \iff
      i \in \{n\ |\ n < j\} \iff
      i < j
  \end{equation*}
  This shows that $\psi$ has the order property, so $\theory$ must be unstable.
\end{proof}

However not
all NIP theories are stable, for example all linear orders, infinite or not, are NIP. The proof of
this requires some machinery which is not relevant to the rest of this project, but the details
can be found in \cite{simon15}.


\begin{exmp}
  The real exponential field $(\R,+,\cdot,0,1,e^x)$ has the NIP. \cite{steinhorn1999} This means
  that for any formula $\phi(x;y)$ there exists some maximum $n$ such that
  \begin{equation*}
    \fulltheory(\R) \cup \{\phi(a_i,b_I)\ |\ i \in I \subseteq \{0,1,\dots,n\}\}
  \end{equation*}
  is satisfiable. We refer to this $n$ as the VC dimension of $\phi(x;y)$. This notion of VC
  dimension is not only relevant in model theory, in machine learning it can be used to measure the
  expressive power of a classification model. One interesting and often studied class of
  classification models comes from feedforward neural networks with sigmoid activation functions. It
  turns out that any such neural network can be expressed as a first order formula in the language
  of exponential fields with its weights as parameters \cite{macintyre93} and as such any
  feedforward neural network with sigmoid activation functions will have finite VC dimension.
\end{exmp}

Interestingly, by modifying the above slightly to consider the complex exponential field, we get a
theory with the IP. This points towards the ``precariousness'' of the NIP, and how even small
changes in the theory may result in losing this property.

\begin{exmp}
  To see why the complex exponential field $(\mathbb{C},+,\cdot,0,1,e^x)$ has the IP, we first see
  how to define the set of integers. For this, notice that the set $\{i,-i\}$ is defined by
  the formula $\phi(x) = x \cdot x = 1$. Then, recall that we have
  \begin{equation*}
    \sin(\theta) = \frac{e^{i\theta} - e^{(-i)\theta}}{2i}
                 = \frac{e^{(-i)\theta} - e^{i\theta}}{2(-i)}
  \end{equation*}
  hence we can define sine using the formula
  \begin{equation*}
    \psi(x,y) = \exists a, \exists b, (a \neq b) \land \phi(a) \land \phi(b) \land
      \left(y = \frac{e^{ax} - e^{bx}}{2a}\right)
  \end{equation*}
  In turn, this allows us to define $2\pi \Z \subseteq \mathbb{\C}$ as the zero set of the sine
  function. Finally, the following formula, which essentially says that as integers, $y$ divides
  $x$, has the IP
  \begin{equation*}
    \varphi(x;y,\pi) = \exists z, \psi(z,0) \land \left(y \cdot \frac{z}{2\pi} = x\right)
  \end{equation*}
  which we show by considering arbitrarily large finite sets. Fix some $n$, then the subsets of
  $\{1,\dots,n\}$ can be put in bijection with $\{1,\dots,n^2\}$ under some $f$. Using $p_k$
  to refer to the $k$th prime, we let
  \begin{equation*}
    b_I = (p_{f(I)}, \pi)
  \end{equation*}
  for each $I \subseteq \{1,\dots,n\}$. Also for each $i \in \{1,\dots,n\}$ we let
  \begin{equation*}
    a_i = \prod_{\substack{J \subseteq \{1,\dots,n\},\\ i \in J}} p_{f(J)}
  \end{equation*}
  With the use of these parameters
  \begin{equation*}
    \C \models \varphi(a_i;b_I)
      \iff p_{f(I)}\ | \prod_{\substack{J \subseteq \{1,\dots,n\},\\ i \in J}} p_{f(J)}
      \iff i \in I
  \end{equation*}
  As this works for arbitrarily $n$, $\varphi(x;y,a)$ must have the IP.
\end{exmp}
